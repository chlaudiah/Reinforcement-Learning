{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> REINFORCEMENT LEARNING ( Q - LEARNING ) </center>\n",
    "Nama: Chlaudiah Julinar\n",
    "<br>\n",
    "NIM: 1301150434\n",
    "<br>\n",
    "Kelas: IF 39 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> A. LAPORAN </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analisis Masalah\n",
    "**Masalah:**\n",
    "<br>\n",
    "Bangunlah sebuah sistem Q-learning untuk menemukan optimum policy sehingga Agent yang berada di posisi Start (1,1) mampu menemukan Goal yang berada di posisi (10,10) dengan mendapatkan Total Reward maksimum pada grid world dalam Figure 1 berikut ini. Data pada Figure 1 dapat dilihat di Ô¨Åle DataTugasML3.txt. Pada kasus ini, Agent hanya bisa melakukan empat aksi: N, E, S, dan W yang secara berurutan menyatakan North (ke atas), East (ke kanan), South (ke bawah), dan West (ke kiri). Anda boleh menggunakan skema apapun dalam mengimplementasikan sebuah episode.\n",
    "<br>\n",
    "*Keterangan Gambar*: Berikut merupakan grid world yang diberikan berukuran 10 x 10, dimana angka-angka didalam kotak menyatakan *reward*,*agen* berada pada *Initial State* = (1,1) dan berakhir pada *Goal State* = (10,10)\n",
    "![Ilustrasi](Grid World.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Masalah:**\n",
    "<br>\n",
    "Masalah yang diharus diselesaikan disini yaitu, menemukan *Total Reward* terbaik yang dihasilkan selama *agent* melakukan learning yaitu berupa cara menuju *Initial State* yaitu (1,1) hingga sampai pada *Goal State* yaitu (10,10). Setiap kali, *agent* selesai melakukan satu kali perjalanan, maka akan dihitung score yang diperoleh dari tiap reward yang didapat selama *agent* melakukan *action* yaitu = up, down, lef, dan right.\n",
    "\n",
    "\n",
    "Sebelum membuat *agent* melakukan learning, terlebih dahulu kita harus menyediakan atau mendefinisikan *environment* dari jalur yang akan dilalui oleh *agent*. Desain dari *environment* akan dijelaskan pada tahap selanjutnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Desain Environment\n",
    "Desain Environment yaitu tahap dimana kita akan mengatur *action* dari *agent* sehingga *agent* dapat memutuskan harus bergerak kearah mana dari *action* yang diberikan. Lalu, kita juga harus mengatur berapa nilai *reward* yang diperoleh jika *agent* melakukan salah satu *action*.\n",
    "#### Mengatur Gerak dari Agent\n",
    "* Saat aksi yang dipilih adalah 0 maka agent akan bergerak kearah atas, artinya nilai dari koordinat y akan bertambah 1 tiap kali agent bergerak ke atas selama belum lebih dari y = 10\n",
    "* Saat aksi yang dipilih adalah 1 maka agent akan bergerak kearah bawah, artinya nilai dari koordinat y akan berkurang 1 tiap kali agent bergerak kebawah selama belum kurang dari y = 0 \n",
    "* Saat aksi yang dipilih adalah 2 maka agent akan bergerak kearah kanan, artinya nilai dari koordinat x akan bertambah 1 tiap kali agent bergerak selama belum lebih dari x = 10\n",
    "* Saat aksi yang dipilih adalah 3 maka agent akan bergerak kearah atas, artinya nilai dari koordinat x akan berkurang 1 tiap kali agent bergerak selama belum kurang dari x = 0\n",
    "\n",
    "#### Mengisi Nilai Reward dari tiap Aksi yang dilakukan\n",
    "* Saat aksi yang dipilih adalah 0 maka agent akan bergerak kearah atas, maka reward yang diperoleh adalah saat agent berada pada koordinat y > 0 dan nilai y akan ditambahkan 1 tiap kali agent bergerak selama belum lebih dari y = 10 dan nilai x tetap\n",
    "* Saat aksi yang dipilih adalah 1 maka agent akan bergerak kearah bawah,maka reward yang diperoleh adalah saat agent berada pada koordinat y < 10 dan nilai y akan dikurangkan 1 tiap kali agent bergerak selama belum kurang dari y = 0 dan nilai x tetap\n",
    "* Saat aksi yang dipilih adalah 2 maka agent akan bergerak kearah kanan, maka reward yang diperoleh adalah saat agent berada pada koordinat x < 10 dan nilai x akan ditambahkan 1 tiap kali agent bergerak selama belum lebih dari x = 10 dan nilai y tetap\n",
    "* Saat aksi yang dipilih adalah 3 maka agent akan bergerak kearah atas, maka reward yang diperoleh adalah saat agent berada pada koordinat x > 0 dan nilai x akan dikurangkan 1 tiap kali agent bergerak selama belum kurang dari x = 0 dan nilai y tetap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> B. LANGKAH - LANGKAH PENYELESAIAN Q-LEARNING </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Membuat Fungsi Utama untuk Update Q-table dengan Bellman Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman(loc,dec,gamma,q):\n",
    "    a = (loc[0],loc[1])\n",
    "    hasil1 = reward_table[a][dec] \n",
    "    d0 = q[(loc[0],loc[1])][0]\n",
    "    d1 = q[(loc[0],loc[1])][1]\n",
    "    d2 = q[(loc[0],loc[1])][2]\n",
    "    d3 = q[(loc[0],loc[1])][3]\n",
    "    hasil2 = max(d0, d1, d2, d3)\n",
    "    hasil = hasil1 + (gamma * hasil2)\n",
    "    return hasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mengatur Action dari Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(loc,dec):\n",
    "    if dec == 0 :\n",
    "        if loc[1] > 0 :\n",
    "            loc[1] = loc[1] - 1\n",
    "    elif dec == 1 :\n",
    "        if loc[1] < 9 :\n",
    "            loc[1] = loc[1] + 1\n",
    "    elif dec == 2 :\n",
    "        if loc[0] < 9 :\n",
    "            loc[0] = loc[0] + 1\n",
    "    elif dec == 3 :\n",
    "        if loc[0] > 0 :\n",
    "            loc[0] = loc[0] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mengatur Nilai Reward dari Tiap Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_up(loc):\n",
    "    global reward\n",
    "    if loc[1] > 0 :\n",
    "        y = loc[1] - 1\n",
    "    else :\n",
    "        return 0\n",
    "    return reward[loc[0]][y]\n",
    "\n",
    "def r_down(loc):\n",
    "    global reward\n",
    "    if loc[1] < 9 :\n",
    "        y = loc[1] + 1\n",
    "    else :\n",
    "        return 0\n",
    "    return reward[loc[0]][y]\n",
    "\n",
    "def r_right(loc):\n",
    "    global reward\n",
    "    if loc[0] < 9 :\n",
    "        x = loc[0] + 1\n",
    "    else :\n",
    "        return 0\n",
    "    return reward[x][loc[1]]\n",
    "\n",
    "def r_left(loc):\n",
    "    global reward\n",
    "    if loc[0] > 0 :\n",
    "        x = loc[0] - 1\n",
    "    else :\n",
    "        return 0\n",
    "    return reward[x][loc[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mempersiapkan Tiap Variable yang Dibutuhkan dan Loading Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.  -3.  -5.  -1.  -3.  -3.  -5.  -5.  -1. 100.]\n",
      " [ -2.  -1.  -1.  -4.  -2.  -5.  -3.  -5.  -5.  -5.]\n",
      " [ -3.  -4.  -4.  -1.  -3.  -5.  -5.  -4.  -3.  -5.]\n",
      " [ -3.  -5.  -2.  -5.  -1.  -4.  -5.  -1.  -3.  -4.]\n",
      " [ -4.  -3.  -3.  -2.  -1.  -1.  -1.  -4.  -3.  -4.]\n",
      " [ -4.  -2.  -5.  -2.  -4.  -5.  -1.  -2.  -2.  -4.]\n",
      " [ -4.  -3.  -2.  -3.  -1.  -3.  -4.  -3.  -1.  -3.]\n",
      " [ -4.  -2.  -5.  -4.  -1.  -4.  -5.  -5.  -2.  -4.]\n",
      " [ -2.  -1.  -1.  -4.  -1.  -3.  -5.  -1.  -4.  -1.]\n",
      " [ -5.  -3.  -1.  -2.  -4.  -3.  -5.  -2.  -2.  -2.]]\n"
     ]
    }
   ],
   "source": [
    "start = [9,0]\n",
    "x = 10\n",
    "y = 10\n",
    "finish = [0,9]\n",
    "initial = [0,0,0,0]\n",
    "q_table = {}\n",
    "reward = np.loadtxt(\"DataTugasML3.txt\")\n",
    "states = []\n",
    "reward_table = {}\n",
    "maks = -99999\n",
    "gamma = 0.5\n",
    "\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Menginputkan Koordinat X dan Y sebagai State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x):\n",
    "    for j in range(y):\n",
    "        states.append((i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Membuat Reward Table berukuran 100 x 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): [0, 0, 0, 0], (0, 1): [0, 0, 0, 0], (0, 2): [0, 0, 0, 0], (0, 3): [0, 0, 0, 0], (0, 4): [0, 0, 0, 0], (0, 5): [0, 0, 0, 0], (0, 6): [0, 0, 0, 0], (0, 7): [0, 0, 0, 0], (0, 8): [0, 0, 0, 0], (0, 9): [0, 0, 0, 0], (1, 0): [0, 0, 0, 0], (1, 1): [0, 0, 0, 0], (1, 2): [0, 0, 0, 0], (1, 3): [0, 0, 0, 0], (1, 4): [0, 0, 0, 0], (1, 5): [0, 0, 0, 0], (1, 6): [0, 0, 0, 0], (1, 7): [0, 0, 0, 0], (1, 8): [0, 0, 0, 0], (1, 9): [0, 0, 0, 0], (2, 0): [0, 0, 0, 0], (2, 1): [0, 0, 0, 0], (2, 2): [0, 0, 0, 0], (2, 3): [0, 0, 0, 0], (2, 4): [0, 0, 0, 0], (2, 5): [0, 0, 0, 0], (2, 6): [0, 0, 0, 0], (2, 7): [0, 0, 0, 0], (2, 8): [0, 0, 0, 0], (2, 9): [0, 0, 0, 0], (3, 0): [0, 0, 0, 0], (3, 1): [0, 0, 0, 0], (3, 2): [0, 0, 0, 0], (3, 3): [0, 0, 0, 0], (3, 4): [0, 0, 0, 0], (3, 5): [0, 0, 0, 0], (3, 6): [0, 0, 0, 0], (3, 7): [0, 0, 0, 0], (3, 8): [0, 0, 0, 0], (3, 9): [0, 0, 0, 0], (4, 0): [0, 0, 0, 0], (4, 1): [0, 0, 0, 0], (4, 2): [0, 0, 0, 0], (4, 3): [0, 0, 0, 0], (4, 4): [0, 0, 0, 0], (4, 5): [0, 0, 0, 0], (4, 6): [0, 0, 0, 0], (4, 7): [0, 0, 0, 0], (4, 8): [0, 0, 0, 0], (4, 9): [0, 0, 0, 0], (5, 0): [0, 0, 0, 0], (5, 1): [0, 0, 0, 0], (5, 2): [0, 0, 0, 0], (5, 3): [0, 0, 0, 0], (5, 4): [0, 0, 0, 0], (5, 5): [0, 0, 0, 0], (5, 6): [0, 0, 0, 0], (5, 7): [0, 0, 0, 0], (5, 8): [0, 0, 0, 0], (5, 9): [0, 0, 0, 0], (6, 0): [0, 0, 0, 0], (6, 1): [0, 0, 0, 0], (6, 2): [0, 0, 0, 0], (6, 3): [0, 0, 0, 0], (6, 4): [0, 0, 0, 0], (6, 5): [0, 0, 0, 0], (6, 6): [0, 0, 0, 0], (6, 7): [0, 0, 0, 0], (6, 8): [0, 0, 0, 0], (6, 9): [0, 0, 0, 0], (7, 0): [0, 0, 0, 0], (7, 1): [0, 0, 0, 0], (7, 2): [0, 0, 0, 0], (7, 3): [0, 0, 0, 0], (7, 4): [0, 0, 0, 0], (7, 5): [0, 0, 0, 0], (7, 6): [0, 0, 0, 0], (7, 7): [0, 0, 0, 0], (7, 8): [0, 0, 0, 0], (7, 9): [0, 0, 0, 0], (8, 0): [0, 0, 0, 0], (8, 1): [0, 0, 0, 0], (8, 2): [0, 0, 0, 0], (8, 3): [0, 0, 0, 0], (8, 4): [0, 0, 0, 0], (8, 5): [0, 0, 0, 0], (8, 6): [0, 0, 0, 0], (8, 7): [0, 0, 0, 0], (8, 8): [0, 0, 0, 0], (8, 9): [0, 0, 0, 0], (9, 0): [0, 0, 0, 0], (9, 1): [0, 0, 0, 0], (9, 2): [0, 0, 0, 0], (9, 3): [0, 0, 0, 0], (9, 4): [0, 0, 0, 0], (9, 5): [0, 0, 0, 0], (9, 6): [0, 0, 0, 0], (9, 7): [0, 0, 0, 0], (9, 8): [0, 0, 0, 0], (9, 9): [0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(x*y) :\n",
    "    reward_table[states[i]] = initial\n",
    "print(reward_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Membangun Environtment pada Reward Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dec in range(0,4):\n",
    "    for i in range(len(reward)):\n",
    "        for j in range(len(reward[0])):\n",
    "            loc = [i,j]\n",
    "            temp = []\n",
    "            temp.clear\n",
    "            temp.append(r_up(loc))\n",
    "            temp.append(r_down(loc))\n",
    "            temp.append(r_right(loc))\n",
    "            temp.append(r_left(loc))\n",
    "            reward_table[(i,j)] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Membuat Q Table dengan Value Seluruhnya adalah 0 berukuran 100 x 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): [0, 0, 0, 0], (0, 1): [0, 0, 0, 0], (0, 2): [0, 0, 0, 0], (0, 3): [0, 0, 0, 0], (0, 4): [0, 0, 0, 0], (0, 5): [0, 0, 0, 0], (0, 6): [0, 0, 0, 0], (0, 7): [0, 0, 0, 0], (0, 8): [0, 0, 0, 0], (0, 9): [0, 0, 0, 0], (1, 0): [0, 0, 0, 0], (1, 1): [0, 0, 0, 0], (1, 2): [0, 0, 0, 0], (1, 3): [0, 0, 0, 0], (1, 4): [0, 0, 0, 0], (1, 5): [0, 0, 0, 0], (1, 6): [0, 0, 0, 0], (1, 7): [0, 0, 0, 0], (1, 8): [0, 0, 0, 0], (1, 9): [0, 0, 0, 0], (2, 0): [0, 0, 0, 0], (2, 1): [0, 0, 0, 0], (2, 2): [0, 0, 0, 0], (2, 3): [0, 0, 0, 0], (2, 4): [0, 0, 0, 0], (2, 5): [0, 0, 0, 0], (2, 6): [0, 0, 0, 0], (2, 7): [0, 0, 0, 0], (2, 8): [0, 0, 0, 0], (2, 9): [0, 0, 0, 0], (3, 0): [0, 0, 0, 0], (3, 1): [0, 0, 0, 0], (3, 2): [0, 0, 0, 0], (3, 3): [0, 0, 0, 0], (3, 4): [0, 0, 0, 0], (3, 5): [0, 0, 0, 0], (3, 6): [0, 0, 0, 0], (3, 7): [0, 0, 0, 0], (3, 8): [0, 0, 0, 0], (3, 9): [0, 0, 0, 0], (4, 0): [0, 0, 0, 0], (4, 1): [0, 0, 0, 0], (4, 2): [0, 0, 0, 0], (4, 3): [0, 0, 0, 0], (4, 4): [0, 0, 0, 0], (4, 5): [0, 0, 0, 0], (4, 6): [0, 0, 0, 0], (4, 7): [0, 0, 0, 0], (4, 8): [0, 0, 0, 0], (4, 9): [0, 0, 0, 0], (5, 0): [0, 0, 0, 0], (5, 1): [0, 0, 0, 0], (5, 2): [0, 0, 0, 0], (5, 3): [0, 0, 0, 0], (5, 4): [0, 0, 0, 0], (5, 5): [0, 0, 0, 0], (5, 6): [0, 0, 0, 0], (5, 7): [0, 0, 0, 0], (5, 8): [0, 0, 0, 0], (5, 9): [0, 0, 0, 0], (6, 0): [0, 0, 0, 0], (6, 1): [0, 0, 0, 0], (6, 2): [0, 0, 0, 0], (6, 3): [0, 0, 0, 0], (6, 4): [0, 0, 0, 0], (6, 5): [0, 0, 0, 0], (6, 6): [0, 0, 0, 0], (6, 7): [0, 0, 0, 0], (6, 8): [0, 0, 0, 0], (6, 9): [0, 0, 0, 0], (7, 0): [0, 0, 0, 0], (7, 1): [0, 0, 0, 0], (7, 2): [0, 0, 0, 0], (7, 3): [0, 0, 0, 0], (7, 4): [0, 0, 0, 0], (7, 5): [0, 0, 0, 0], (7, 6): [0, 0, 0, 0], (7, 7): [0, 0, 0, 0], (7, 8): [0, 0, 0, 0], (7, 9): [0, 0, 0, 0], (8, 0): [0, 0, 0, 0], (8, 1): [0, 0, 0, 0], (8, 2): [0, 0, 0, 0], (8, 3): [0, 0, 0, 0], (8, 4): [0, 0, 0, 0], (8, 5): [0, 0, 0, 0], (8, 6): [0, 0, 0, 0], (8, 7): [0, 0, 0, 0], (8, 8): [0, 0, 0, 0], (8, 9): [0, 0, 0, 0], (9, 0): [0, 0, 0, 0], (9, 1): [0, 0, 0, 0], (9, 2): [0, 0, 0, 0], (9, 3): [0, 0, 0, 0], (9, 4): [0, 0, 0, 0], (9, 5): [0, 0, 0, 0], (9, 6): [0, 0, 0, 0], (9, 7): [0, 0, 0, 0], (9, 8): [0, 0, 0, 0], (9, 9): [0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(x*y) :\n",
    "    q_table[states[i]] = initial\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score =  -440.0\n",
      "score =  -583.0\n",
      "score =  -723.0\n",
      "score =  -859.0\n",
      "score =  -673.0\n",
      "score =  -630.0\n",
      "score =  -623.0\n",
      "score =  -862.0\n",
      "score =  -450.0\n",
      "score =  -2568.0\n",
      "score =  -2257.0\n",
      "score =  -898.0\n",
      "score =  -625.0\n",
      "score =  -2530.0\n",
      "score =  -1590.0\n",
      "score =  -394.0\n",
      "score =  -366.0\n",
      "score =  -1153.0\n",
      "score =  -141.0\n",
      "score =  -98.0\n",
      "score =  -1238.0\n",
      "score =  -1036.0\n",
      "score =  -1873.0\n",
      "score =  -1451.0\n",
      "score =  -2399.0\n",
      "score =  -778.0\n",
      "score =  -3658.0\n",
      "score =  -4017.0\n",
      "score =  -1319.0\n",
      "score =  -480.0\n",
      "score =  -581.0\n",
      "score =  -770.0\n",
      "score =  -1369.0\n",
      "score =  -1961.0\n",
      "score =  -502.0\n",
      "score =  -695.0\n",
      "score =  -190.0\n",
      "score =  -1322.0\n",
      "score =  -2697.0\n",
      "score =  -3120.0\n",
      "score =  -1469.0\n",
      "score =  -2790.0\n",
      "score =  -3038.0\n",
      "score =  -187.0\n",
      "score =  -775.0\n",
      "score =  -2487.0\n",
      "score =  -1050.0\n",
      "score =  -1238.0\n",
      "score =  -3356.0\n",
      "score =  -13.0\n",
      "score =  -2749.0\n",
      "score =  -1229.0\n",
      "score =  -639.0\n",
      "score =  -331.0\n",
      "score =  -5560.0\n",
      "score =  -1502.0\n",
      "score =  -2262.0\n",
      "score =  -2988.0\n",
      "score =  -1662.0\n",
      "score =  -635.0\n",
      "score =  -2873.0\n",
      "score =  -806.0\n",
      "score =  -647.0\n",
      "score =  -484.0\n",
      "score =  -14.0\n",
      "score =  -918.0\n",
      "score =  -845.0\n",
      "score =  -1835.0\n",
      "score =  -6859.0\n",
      "score =  -570.0\n",
      "score =  -811.0\n",
      "score =  -90.0\n",
      "score =  -473.0\n",
      "score =  48.0\n",
      "score =  -2257.0\n",
      "score =  -96.0\n",
      "score =  -974.0\n",
      "score =  -394.0\n",
      "score =  -803.0\n",
      "score =  -3138.0\n",
      "score =  -1441.0\n",
      "score =  -1569.0\n",
      "score =  -3419.0\n",
      "score =  -562.0\n",
      "score =  -5137.0\n",
      "score =  -1973.0\n",
      "score =  -841.0\n",
      "score =  -216.0\n",
      "score =  -1829.0\n",
      "score =  -331.0\n",
      "score =  -362.0\n",
      "score =  -148.0\n",
      "score =  -1374.0\n",
      "score =  -228.0\n",
      "score =  -1977.0\n",
      "score =  -491.0\n",
      "score =  -873.0\n",
      "score =  -1484.0\n",
      "score =  -1805.0\n",
      "score =  -275.0\n",
      "\n",
      "Maximum:  48.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    score = 0\n",
    "    q = q_table\n",
    "    end = False\n",
    "    loc = [9,0]\n",
    "    while end == False:\n",
    "        # up = 0 , down = 1 , right = 2 ,left = 3\n",
    "        dec = random.randint(0,3)\n",
    "        bell = bellman(loc,dec,gamma,q)\n",
    "        q[(loc[0],loc[1])][dec] = bell\n",
    "        if dec == 0 : \n",
    "            move(loc,dec)\n",
    "            score = score + r_up(loc)\n",
    "        elif dec == 1:\n",
    "            move(loc,dec)\n",
    "            score = score + r_down(loc)\n",
    "        elif dec == 2:\n",
    "            move(loc,dec)\n",
    "            score = score + r_right(loc)\n",
    "        elif dec == 3 :\n",
    "            move(loc,dec)\n",
    "            score = score + r_left(loc)\n",
    "\n",
    "        if loc[0] == finish[0] and loc[1] == finish[1] :\n",
    "            end = True\n",
    "    print(\"score = \",score)\n",
    "    if maks < score :\n",
    "        maks = score  \n",
    "    q_table = q\n",
    "\n",
    "print()\n",
    "print(\"Maximum: \",maks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> C. HASIL EKSPERIMEN </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil eksperimen yang diperoleh adalah nilai total reward paling bagus selama melakukan eksperimen Q-Learning pada *agent* adalah 48.0 dengan percobaan melakukan episode sebanyak 100 kali episode. Hal ini menyatakan bahwa, *agent* dengan 100 kali episode belum cukup baik memahami environment yang dibuat.\n",
    "\n",
    "Hal ini terjadi dikarenakan beberapa kemungkinan yang saya lakukan pada pemrograman. Beberapa hal tersebut saya perkirakan adalah:\n",
    "1. Aksi yang akan dilakukan diputuskan secara random (dapat dilihat pada bagian 10. Q-Learning)\n",
    "2. Pada saat perhitungan score dengan reward yang diperoleh (dapat dilihat pada bagian 10. Q-Learning)\n",
    "3. Pada penentuan reward pada reward table sebagai environment yang harus dipelajari oleh *agent* (dapat dilihat pada bagian 8. Membangun environment pada reward table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
